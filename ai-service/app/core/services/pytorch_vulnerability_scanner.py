"""
Advanced PyTorch-based Vulnerability Scanner
Uses deep learning models for comprehensive vulnerability detection
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional
import re
import ast
import hashlib
import json
from datetime import datetime
from pathlib import Path
import logging
from dataclasses import dataclass
from enum import Enum

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VulnerabilityType(Enum):
    """Enumeration of vulnerability types"""
    SQL_INJECTION = "sql_injection"
    XSS = "xss"
    COMMAND_INJECTION = "command_injection"
    PATH_TRAVERSAL = "path_traversal"
    INSECURE_DESERIALIZATION = "insecure_deserialization"
    HARDCODED_CREDENTIALS = "hardcoded_credentials"
    WEAK_CRYPTOGRAPHY = "weak_cryptography"
    INSECURE_RANDOM = "insecure_random"
    BUFFER_OVERFLOW = "buffer_overflow"
    RACE_CONDITION = "race_condition"

@dataclass
class VulnerabilityResult:
    """Data class for vulnerability detection results"""
    file_path: str
    vulnerability_type: VulnerabilityType
    severity: str  # 'critical', 'high', 'medium', 'low'
    confidence: float
    line_number: int
    code_snippet: str
    description: str
    recommendation: str
    cwe_id: Optional[str] = None
    cvss_score: Optional[float] = None

class CodeDataset(Dataset):
    """Dataset class for code vulnerability analysis"""
    
    def __init__(self, code_samples: List[Dict[str, Any]], tokenizer=None):
        self.code_samples = code_samples
        self.tokenizer = tokenizer
        
    def __len__(self):
        return len(self.code_samples)
    
    def __getitem__(self, idx):
        sample = self.code_samples[idx]
        features = self.extract_features(sample['code'], sample.get('file_path', ''))
        label = sample.get('label', 0)
        return torch.FloatTensor(features), torch.LongTensor([label])
    
    def extract_features(self, code: str, file_path: str = "") -> np.ndarray:
        """Extract comprehensive features from code"""
        features = []
        
        # Basic code metrics
        lines = code.split('\n')
        features.append(len(lines))  # Number of lines
        features.append(len(code))  # Total characters
        features.append(sum(len(line) for line in lines) / max(len(lines), 1))  # Average line length
        
        # Token-level features
        tokens = re.findall(r'\w+', code)
        features.append(len(tokens))  # Number of tokens
        features.append(len(set(tokens)))  # Unique tokens
        
        # Security-sensitive keywords
        security_keywords = [
            'password', 'secret', 'key', 'token', 'auth', 'login', 'exec',
            'eval', 'system', 'shell', 'cmd', 'sql', 'query', 'database',
            'file', 'read', 'write', 'upload', 'download', 'include', 'require',
            'crypto', 'encrypt', 'decrypt', 'hash', 'random', 'seed'
        ]
        
        for keyword in security_keywords:
            features.append(code.lower().count(keyword))
        
        # Code structure features
        features.append(code.count('('))  # Function calls
        features.append(code.count('{'))  # Code blocks
        features.append(code.count(';'))  # Statements
        features.append(code.count('"') + code.count("'"))  # String literals
        features.append(code.count('//') + code.count('#'))  # Comments
        
        # Risk patterns
        risk_patterns = [
            'http://', 'https://', 'ftp://',  # URLs
            '<?php', '<%', '<script',  # Script tags
            'SELECT', 'INSERT', 'UPDATE', 'DELETE',  # SQL
            'eval(', 'exec(', 'system(', 'shell_exec(',  # Dangerous functions
            'md5(', 'sha1(', 'crc32(',  # Weak crypto
            'rand(', 'random(', 'mt_rand(',  # Weak random
        ]
        
        for pattern in risk_patterns:
            features.append(code.upper().count(pattern))
        
        # File extension features
        file_extensions = ['.php', '.js', '.py', '.rb', '.pl', '.sh', '.bat', '.java', '.cpp', '.c']
        ext_features = [1 if file_path.endswith(ext) else 0 for ext in file_extensions]
        features.extend(ext_features)
        
        # Entropy-based features
        features.append(self.calculate_entropy(code))
        features.append(self.calculate_entropy(code.replace(' ', '').replace('\n', '')))
        
        # Normalize and pad to fixed size
        feature_array = np.array(features, dtype=np.float32)
        if len(feature_array) < 200:
            feature_array = np.pad(feature_array, (0, 200 - len(feature_array)))
        elif len(feature_array) > 200:
            feature_array = feature_array[:200]
        
        return feature_array
    
    def calculate_entropy(self, text: str) -> float:
        """Calculate Shannon entropy of text"""
        if not text:
            return 0.0
        
        char_counts = {}
        for char in text:
            char_counts[char] = char_counts.get(char, 0) + 1
        
        entropy = 0.0
        text_len = len(text)
        for count in char_counts.values():
            probability = count / text_len
            entropy -= probability * np.log2(probability)
        
        return entropy

class VulnerabilityClassifier(nn.Module):
    """Deep neural network for vulnerability classification"""
    
    def __init__(self, input_size: int = 200, hidden_sizes: List[int] = [512, 256, 128], num_classes: int = 10):
        super(VulnerabilityClassifier, self).__init__()
        
        layers = []
        current_size = input_size
        
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(current_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            current_size = hidden_size
        
        layers.append(nn.Linear(current_size, num_classes))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)

class AttentionVulnerabilityDetector(nn.Module):
    """Attention-based vulnerability detector for sequence analysis"""
    
    def __init__(self, vocab_size: int = 10000, embedding_dim: int = 128, hidden_dim: int = 256, num_classes: int = 10):
        super(AttentionVulnerabilityDetector, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=8)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length)
        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)
        
        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim * 2)
        
        # Apply attention
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # Global average pooling
        pooled = torch.mean(attn_out, dim=1)  # (batch_size, hidden_dim * 2)
        
        return self.classifier(pooled)

class PyTorchVulnerabilityScanner:
    """Advanced PyTorch-based vulnerability scanner"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {self.device}")
        
        # Initialize models
        self.feature_classifier = VulnerabilityClassifier().to(self.device)
        self.sequence_detector = AttentionVulnerabilityDetector().to(self.device)
        
        # Load pre-trained models if available
        if model_path and Path(model_path).exists():
            self.load_models(model_path)
        
        # Vulnerability patterns and rules
        self.vulnerability_patterns = self._load_vulnerability_patterns()
        self.cwe_mapping = self._load_cwe_mapping()
        
        # Model training state
        self.is_trained = False
        self.optimizer = None
        self.criterion = None
        
    def _load_vulnerability_patterns(self) -> Dict[str, List[Dict[str, Any]]]:
        """Load vulnerability detection patterns"""
        return {
            'sql_injection': [
                {
                    'pattern': r'(SELECT|INSERT|UPDATE|DELETE|DROP|CREATE|ALTER)\s+.*\s+FROM\s+\w+',
                    'severity': 'critical',
                    'description': 'SQL injection vulnerability detected',
                    'cwe_id': 'CWE-89'
                },
                {
                    'pattern': r'["\']?\s*\+\s*["\']?\s*\+\s*["\']?\w+["\']?',
                    'severity': 'high',
                    'description': 'Potential SQL string concatenation',
                    'cwe_id': 'CWE-89'
                }
            ],
            'xss': [
                {
                    'pattern': r'<script[^>]*>.*?</script>',
                    'severity': 'critical',
                    'description': 'Cross-site scripting vulnerability',
                    'cwe_id': 'CWE-79'
                },
                {
                    'pattern': r'innerHTML\s*=|outerHTML\s*=|document\.write',
                    'severity': 'high',
                    'description': 'DOM-based XSS vulnerability',
                    'cwe_id': 'CWE-79'
                }
            ],
            'command_injection': [
                {
                    'pattern': r'(exec|system|shell_exec|passthru|eval)\s*\(',
                    'severity': 'critical',
                    'description': 'Command injection vulnerability',
                    'cwe_id': 'CWE-78'
                }
            ],
            'hardcoded_credentials': [
                {
                    'pattern': r'(password|passwd|pwd)\s*=\s*["\'][^"\']+["\']',
                    'severity': 'high',
                    'description': 'Hardcoded password detected',
                    'cwe_id': 'CWE-798'
                },
                {
                    'pattern': r'(api_key|apikey|secret_key|private_key)\s*=\s*["\'][^"\']+["\']',
                    'severity': 'critical',
                    'description': 'Hardcoded API key detected',
                    'cwe_id': 'CWE-798'
                }
            ],
            'weak_cryptography': [
                {
                    'pattern': r'(md5|sha1|crc32)\s*\(',
                    'severity': 'medium',
                    'description': 'Weak cryptographic hash function',
                    'cwe_id': 'CWE-327'
                },
                {
                    'pattern': r'(des|rc4|rc2)\s*\(',
                    'severity': 'high',
                    'description': 'Weak encryption algorithm',
                    'cwe_id': 'CWE-327'
                }
            ],
            'path_traversal': [
                {
                    'pattern': r'\.\./|\.\.\\',
                    'severity': 'high',
                    'description': 'Path traversal vulnerability',
                    'cwe_id': 'CWE-22'
                }
            ]
        }
    
    def _load_cwe_mapping(self) -> Dict[str, Dict[str, Any]]:
        """Load CWE to CVSS score mapping"""
        return {
            'CWE-89': {'name': 'SQL Injection', 'cvss': 9.8},
            'CWE-79': {'name': 'Cross-site Scripting', 'cvss': 8.1},
            'CWE-78': {'name': 'Command Injection', 'cvss': 9.8},
            'CWE-798': {'name': 'Hardcoded Credentials', 'cvss': 7.5},
            'CWE-327': {'name': 'Weak Cryptography', 'cvss': 5.9},
            'CWE-22': {'name': 'Path Traversal', 'cvss': 7.5},
            'CWE-20': {'name': 'Input Validation', 'cvss': 7.5},
            'CWE-310': {'name': 'Cryptographic Issues', 'cvss': 7.5},
            'CWE-119': {'name': 'Buffer Overflow', 'cvss': 9.8},
            'CWE-362': {'name': 'Race Condition', 'cvss': 6.2}
        }
    
    def scan_file(self, file_path: str, content: str) -> List[VulnerabilityResult]:
        """Scan a single file for vulnerabilities"""
        vulnerabilities = []
        
        try:
            # Pattern-based detection
            pattern_vulns = self._detect_patterns(file_path, content)
            vulnerabilities.extend(pattern_vulns)
            
            # ML-based detection
            ml_vulns = self._ml_detect_vulnerabilities(file_path, content)
            vulnerabilities.extend(ml_vulns)
            
            # Remove duplicates and sort by severity
            vulnerabilities = self._deduplicate_vulnerabilities(vulnerabilities)
            vulnerabilities = sorted(vulnerabilities, key=lambda x: self._severity_score(x.severity), reverse=True)
            
        except Exception as e:
            logger.error(f"Error scanning file {file_path}: {str(e)}")
        
        return vulnerabilities
    
    def scan_directory(self, directory_path: str, file_extensions: List[str] = None) -> Dict[str, List[VulnerabilityResult]]:
        """Scan entire directory for vulnerabilities"""
        if file_extensions is None:
            file_extensions = ['.php', '.js', '.py', '.rb', '.pl', '.sh', '.bat', '.java', '.cpp', '.c']
        
        results = {}
        directory = Path(directory_path)
        
        for file_path in directory.rglob('*'):
            if file_path.is_file() and file_path.suffix in file_extensions:
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    vulnerabilities = self.scan_file(str(file_path), content)
                    if vulnerabilities:
                        results[str(file_path)] = vulnerabilities
                        
                except Exception as e:
                    logger.error(f"Error reading file {file_path}: {str(e)}")
        
        return results
    
    def _detect_patterns(self, file_path: str, content: str) -> List[VulnerabilityResult]:
        """Detect vulnerabilities using pattern matching"""
        vulnerabilities = []
        lines = content.split('\n')
        
        for vuln_type, patterns in self.vulnerability_patterns.items():
            for pattern_info in patterns:
                pattern = re.compile(pattern_info['pattern'], re.IGNORECASE | re.MULTILINE)
                
                for match in pattern.finditer(content):
                    line_num = content[:match.start()].count('\n') + 1
                    line_content = lines[line_num - 1] if line_num <= len(lines) else ""
                    
                    vulnerability = VulnerabilityResult(
                        file_path=file_path,
                        vulnerability_type=VulnerabilityType(vuln_type),
                        severity=pattern_info['severity'],
                        confidence=0.9,  # High confidence for pattern matches
                        line_number=line_num,
                        code_snippet=line_content.strip(),
                        description=pattern_info['description'],
                        recommendation=self._get_recommendation(vuln_type),
                        cwe_id=pattern_info.get('cwe_id'),
                        cvss_score=self._get_cvss_score(pattern_info.get('cwe_id'))
                    )
                    vulnerabilities.append(vulnerability)
        
        return vulnerabilities
    
    def _ml_detect_vulnerabilities(self, file_path: str, content: str) -> List[VulnerabilityResult]:
        """Detect vulnerabilities using machine learning models"""
        vulnerabilities = []
        
        if not self.is_trained:
            # Use rule-based fallback if model not trained
            return vulnerabilities
        
        try:
            # Extract features
            dataset = CodeDataset([{'code': content, 'file_path': file_path}])
            features, _ = dataset[0]
            features = features.unsqueeze(0).to(self.device)
            
            # Predict using feature classifier
            with torch.no_grad():
                self.feature_classifier.eval()
                predictions = self.feature_classifier(features)
                probabilities = F.softmax(predictions, dim=1)
                
                # Get top predictions
                top_probs, top_indices = torch.topk(probabilities, 3)
                
                for i, (prob, idx) in enumerate(zip(top_probs[0], top_indices[0])):
                    if prob > 0.5:  # Confidence threshold
                        vuln_type = list(VulnerabilityType)[idx.item()]
                        severity = self._confidence_to_severity(prob.item())
                        
                        vulnerability = VulnerabilityResult(
                            file_path=file_path,
                            vulnerability_type=vuln_type,
                            severity=severity,
                            confidence=prob.item(),
                            line_number=1,  # ML detection doesn't provide line numbers
                            code_snippet=content[:100] + "..." if len(content) > 100 else content,
                            description=f"ML-detected {vuln_type.value} vulnerability",
                            recommendation=self._get_recommendation(vuln_type.value),
                            cvss_score=self._severity_to_cvss(severity)
                        )
                        vulnerabilities.append(vulnerability)
        
        except Exception as e:
            logger.error(f"Error in ML detection: {str(e)}")
        
        return vulnerabilities
    
    def train_models(self, training_data: List[Dict[str, Any]], epochs: int = 10, batch_size: int = 32):
        """Train the ML models on labeled vulnerability data"""
        logger.info("Starting model training...")
        
        # Create dataset and dataloader
        dataset = CodeDataset(training_data)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # Setup training
        self.optimizer = optim.Adam(self.feature_classifier.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()
        
        # Training loop
        self.feature_classifier.train()
        for epoch in range(epochs):
            total_loss = 0
            correct = 0
            total = 0
            
            for batch_features, batch_labels in dataloader:
                batch_features = batch_features.to(self.device)
                batch_labels = batch_labels.squeeze().to(self.device)
                
                self.optimizer.zero_grad()
                outputs = self.feature_classifier(batch_features)
                loss = self.criterion(outputs, batch_labels)
                
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += batch_labels.size(0)
                correct += (predicted == batch_labels).sum().item()
            
            accuracy = 100 * correct / total
            logger.info(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}, Accuracy: {accuracy:.2f}%")
        
        self.is_trained = True
        logger.info("Model training completed!")
    
    def save_models(self, save_path: str):
        """Save trained models"""
        torch.save({
            'feature_classifier': self.feature_classifier.state_dict(),
            'sequence_detector': self.sequence_detector.state_dict(),
            'is_trained': self.is_trained
        }, save_path)
        logger.info(f"Models saved to {save_path}")
    
    def load_models(self, load_path: str):
        """Load pre-trained models"""
        checkpoint = torch.load(load_path, map_location=self.device)
        self.feature_classifier.load_state_dict(checkpoint['feature_classifier'])
        self.sequence_detector.load_state_dict(checkpoint['sequence_detector'])
        self.is_trained = checkpoint.get('is_trained', False)
        logger.info(f"Models loaded from {load_path}")
    
    def _deduplicate_vulnerabilities(self, vulnerabilities: List[VulnerabilityResult]) -> List[VulnerabilityResult]:
        """Remove duplicate vulnerabilities"""
        seen = set()
        unique_vulns = []
        
        for vuln in vulnerabilities:
            # Create unique key based on type, line, and snippet
            key = (vuln.vulnerability_type, vuln.line_number, vuln.code_snippet[:50])
            if key not in seen:
                seen.add(key)
                unique_vulns.append(vuln)
        
        return unique_vulns
    
    def _severity_score(self, severity: str) -> int:
        """Convert severity to numeric score for sorting"""
        scores = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}
        return scores.get(severity, 0)
    
    def _confidence_to_severity(self, confidence: float) -> str:
        """Convert confidence score to severity level"""
        if confidence >= 0.9:
            return 'critical'
        elif confidence >= 0.7:
            return 'high'
        elif confidence >= 0.5:
            return 'medium'
        else:
            return 'low'
    
    def _severity_to_cvss(self, severity: str) -> float:
        """Convert severity to CVSS score"""
        cvss_scores = {'critical': 9.5, 'high': 7.5, 'medium': 5.5, 'low': 3.5}
        return cvss_scores.get(severity, 5.0)
    
    def _get_recommendation(self, vuln_type: str) -> str:
        """Get remediation recommendation for vulnerability type"""
        recommendations = {
            'sql_injection': 'Use parameterized queries or prepared statements to prevent SQL injection',
            'xss': 'Sanitize user input and use output encoding to prevent XSS attacks',
            'command_injection': 'Avoid executing user input as system commands. Use allow-lists for allowed commands',
            'path_traversal': 'Validate and sanitize file paths. Use allow-lists for permitted directories',
            'hardcoded_credentials': 'Store credentials in environment variables or secure configuration files',
            'weak_cryptography': 'Use strong cryptographic algorithms (AES-256, SHA-256, etc.)',
            'insecure_random': 'Use cryptographically secure random number generators',
            'buffer_overflow': 'Use bounds checking and safe string manipulation functions',
            'race_condition': 'Implement proper synchronization mechanisms and atomic operations'
        }
        return recommendations.get(vuln_type, 'Review and fix the identified security issue')
    
    def _get_cvss_score(self, cwe_id: Optional[str]) -> Optional[float]:
        """Get CVSS score for CWE ID"""
        if cwe_id and cwe_id in self.cwe_mapping:
            return self.cwe_mapping[cwe_id]['cvss']
        return None
    
    def generate_report(self, scan_results: Dict[str, List[VulnerabilityResult]]) -> Dict[str, Any]:
        """Generate comprehensive vulnerability report"""
        total_vulnerabilities = sum(len(vulns) for vulns in scan_results.values())
        
        # Count by severity
        severity_counts = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        type_counts = {}
        
        all_vulnerabilities = []
        for file_path, vulnerabilities in scan_results.items():
            for vuln in vulnerabilities:
                severity_counts[vuln.severity] += 1
                type_counts[vuln.vulnerability_type.value] = type_counts.get(vuln.vulnerability_type.value, 0) + 1
                all_vulnerabilities.append(vuln)
        
        # Calculate risk score
        risk_score = (
            severity_counts['critical'] * 10 +
            severity_counts['high'] * 7 +
            severity_counts['medium'] * 4 +
            severity_counts['low'] * 1
        )
        
        return {
            'scan_date': datetime.now().isoformat(),
            'total_files_scanned': len(scan_results),
            'total_vulnerabilities': total_vulnerabilities,
            'severity_breakdown': severity_counts,
            'type_breakdown': type_counts,
            'risk_score': risk_score,
            'files_with_vulnerabilities': list(scan_results.keys()),
            'most_common_vulnerability': max(type_counts.items(), key=lambda x: x[1])[0] if type_counts else None,
            'highest_risk_file': max(scan_results.items(), key=lambda x: sum(self._severity_score(v.severity) for v in x[1]))[0] if scan_results else None
        }

# Example usage and testing
if __name__ == "__main__":
    # Initialize scanner
    scanner = PyTorchVulnerabilityScanner()
    
    # Example vulnerable code
    vulnerable_code = """
    <?php
    $username = $_GET['username'];
    $password = $_GET['password'];
    
    // SQL injection vulnerability
    $query = "SELECT * FROM users WHERE username = '$username' AND password = '$password'";
    $result = mysqli_query($conn, $query);
    
    // Hardcoded credentials
    $api_key = "sk-1234567890abcdef";
    
    // Command injection
    $cmd = "ls -la " . $_GET['dir'];
    system($cmd);
    ?>
    """
    
    # Scan the code
    vulnerabilities = scanner.scan_file("test.php", vulnerable_code)
    
    # Print results
    print(f"Found {len(vulnerabilities)} vulnerabilities:")
    for vuln in vulnerabilities:
        print(f"- {vuln.vulnerability_type.value}: {vuln.description} (Line {vuln.line_number})")
        print(f"  Severity: {vuln.severity}, Confidence: {vuln.confidence:.2f}")
        print(f"  Recommendation: {vuln.recommendation}")
        print()
